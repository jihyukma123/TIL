# 뉴럴 네트워크??

뉴럴? 네트워크??

뭔지 알아보자.

뉴런? 숫자를 가지고 있는 것 이라고 생각하면 됨.

28\*28=784픽셀로 이루어진 숫자 사진을 생각해보자.

각 픽셀은,0(white) ~ 1.0(black)의 범위를 가지는, 밝기를 나타내는 값을 가지고 있음.

뉴런 안에 있는 이 밝기 값을 'activation'이라고 함.

784개의 픽셀이 첫 번째 레이어의 인풋으로 입력됨.

그리고 마지막 레이어는 10개의 뉴런으로 구성됨(0~9의 숫자)

각 뉴런이 가지는 값이, input으로 전달된 pixel들이 보여주고 있는 숫자가 해당 숫자일 가능성을 의미함.

입력레이어와 출력 레이어 사이에 있는 레이어들을 은닉층이라고 함.

지금은 각각 16개의 뉴런으로 구성된 2개의 은닉층이 있다고 가정(분석할 때는 2개의 레이어를 많이 사용한다고 함)

# 왜 레이어 쓰는지?

레이어는 뭔지?

은닉층은 뭘 하는건지?

레이어를 활용해서 복잡한 패턴들을 파악하는 것을 기대하는 것임.

~을 한다 이렇게 말하는게 아니라, 우리가 어떤 의도로 이렇게 뉴럴 네트워크를 구성하는지에 대해서 이야기하는게 인상깊네. 여러 은닉층을 통해서 패턴을 파악하고 식별하는 것이 목적이다.
(진짜 되는지는 몰라도...)

패턴 파악은 여러 AI 모델들에 공통적으로 적용되는 동작 원리임.

각 노드에 대해서 activation을 구한다고 했잖아. 활성화 함수에 가중치의 합을 전달해서..

이 활성화 함수의 결과를 0~1 사이의 시그모이드로 해서 활성화여부를 결정하는데,

활성화 여부는 사실 얼마나 특정 패턴과 맞는지의 문제임.

weight -> 어떤 픽셀의 패턴이 다음 레이어로 활성화되어서 넘어가야하는 유의미한 패턴인지 알아내는 용도
bias -> weight 들로 정해진 activation값이 어느정도가 되었을 때, '유의미하다'라고 정해주는 것.

그치. 예를 들어서, 모든걸 절대치로 계산해버리면, 어떤 건 다 10 이상이 되어버려서 항상 활성화될거아니야. 그러니까 그런 케이스에는 유의미한 값을 10 이상으로 설정하던가 하는 처리가 필요하지.

앞서 말했던, 784개의 뉴런을 활용해서 다음 레이어의 뉴런 각각의 activation을 정할 때, 각 뉴런마다 앞단계의 모든 뉴런의 weight를 다 합산해서 계산하고, 거기다가 각 뉴런 당 bias도 하나씩 있음.
즉 뉴런마다 어느정도 값이 되었을 때 유의미한 것으로 판단할지 기준치가 다름.

우리가 이번에 공부하는 모델은 총 13002개의 파라미터를 가지고 있고, 이 파라미터들을 조금씩 조정해가면서 최적의 weight와 bias를 찾는거임.

훈련이라는 것은 결국 이렇게 구성된 뉴럴네트워크가, 의도된 패턴인식을 잘 수행할 수 있는 모델이 되도록 파라미터를 조정해가면서 최적의 weight와 bias를 찾는 것.

훈련 -> 적절한 weight와 bias를 찾는 것. AHA!

이걸 컴퓨터가 자동으로 계산해줌.

뉴런은 숫자라고 했는데, 더 정확하게 말하면 뉴런은 함수다.

어떤 함수인가? 그냥 활성화 함수라고 생각하면 됨.

이전 층의 모든 뉴런의 출력을 받아서, 0~1사이의 숫자를 출력하는 함수

마찬가지로 전체 뉴럴 네트워크도 사실상 하나의 함수임.

784개의 숫자를 입력으로 받아서, 10개의 숫자를 출력하는 함수

엄청나게 복잡한 함수지만....

시그모이드 함수는 참고로 이제는 그렇게 많이 쓰이지 않는다고 함.

요즘은 ReLU 기반의 활성화 함수를 많이 쓴다고 함.

시그모이드는 기울기 소실 등 문제가 발생함. ReLU는 훨씬 그런 잘 되고 그래서 대부분 많이 사용한다고 함.
