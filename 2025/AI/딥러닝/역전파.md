# 역전파

일단, 가장 마지막 레이어인 출력층의 10개의 뉴런이 각각 가지는 값을 보자.

2에 해당되는 이미지를 입력했는데, 출력층에서 2에 해당되는 뉴런의 값이 0.2가 나왔다고 생각해보자.

그러면 우리의 목표는 이 값을 올리는 것.(물론 전체적으로 코스트가 낮아지는 방향으로 weight와 bias를 낮추는 것이겠지만 단일 뉴런값만 놓고 보면 얘를 올리거나 낮춰야함)

그러면 해당 뉴런은 어떻게 계산된 값이지?

이전 레이어에서 모든 뉴런의 값과 가중치 그리고 bias를 활용한 시그모이드/ReLU와 같은 활성화 함수를 통과한 결과값임.

그러면 0.2라는 활성화 함수 결과값( === 최종 레이어 2 뉴런값)을 올리는 방법은 3가지라고 함.

- bias 값을 높이거나
- weight값을 높이거나
- 활성화값을 바꾸거나

이 때 이전 레이어의 뉴런별로 하나씩 존재하는 가중치값이 몇으로 바뀌어야 하는지와 관련해서, 모든 경우에 동일하게 적용되는 법칙이 있음(이해해야되는 포인트). 그것은 바로....

> 이전 층에서 가장 활발했던 뉴런(활성화 값이 높은 뉴런)과 연결되어 있는 가중치가 변화되었을 때 전체 값에 영향을 주는 정도가 크다는 점.

활성화 함수 결과값에 영향을 주려면, 애초에 활성화 값이 큰 뉴런의 가중치를 키우는게 영향이 크다.

여기서 역전파라는 개념이 등장함.

Gemini는 이렇게 설명했는데 조금 더 직관적으로 이해가 되었음.

> 엉망인 결과를 보고, 그 오류에 가장 큰 책임이 있는 신경망 내부의 가중치들을 찾아서 수정하는 것이 역전파의 목적

무턱대고 수정하는게 아니라 오류에 가장 기여를 많이 한 놈을 찾아서 팬다 이거네. 주동자를 찾아라!

역전파가 잘 동작하려면 충분한 양의 데이터가 필요하다고 함
