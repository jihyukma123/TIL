# 훈련은 어떻게 하는걸까??

훈련을 하기 위해서는 뭐가 필요한가? -> 데이터가 필요하다.

데이터를 기반으로 훈련을 하는데, 우리가 이 영상 시리즈에서 공부하기 위해서 보는 숫자 이미지를 기준으로 숫자를 판별하는 모델의 데이터는 손글씨 숫자임.

손글씨 숫자와, 그 숫자에 해당되는 레이블(어떤 숫자인지)가 하나의 data set임.

어느정도 훈련시킨 다음에, 테스트셋을 기준으로 숫자를 얼마나 잘 맞추는지를 평가하면, 모델이 얼마나 잘 배웠는지 알 수 있음.

참고로 이 모델을 훈련시키기 위해서 사용된 데이터는 `MNIST 데이터베이스`임.

신경망이 실제로 동작하는 걸 알게 되면, `기계가 학습한다`라는 표현이 사실 좀 이상한 표현인걸 알게 됨.

사실상 그냥 미적분일 뿐인데...(더 끔찍하네 ㅎ)

기계학습은 그냥 미적분 예제에 더 가깝다고 함.

앞서 배웠던 것처럼, 모델에서 입력층을 제외하면 모든 층의 특정 뉴런은 이전 층의 모든 `뉴런의 값\*가중치 + bias`로 구성된 값을 가짐.

근데 그러면 최초에는 이 값이 어떻게 설정돼? 이 값이 랜덤하게 설정되어 있다고 함.
(읭? 근데 왜 일정한 값이나 0 같은 값으로 초기화하지 않고 랜덤값으로 초기화할까? -> 동일한 값으로 초기화하면, 모든 뉴런이 동일한 값을 가지게 되기 때문에 이게 밸런스가 영원히 깨지지 않음. 학습을 할 기준이 없어지지.)

랜덤하게 준 값에 데이터를 입력하면 어떻게 될까?
-> 쓰레기값이 출력이 됨. 이게 얼마나 정답이랑 차이가 있는지를 `cost`라고 하는거임.

그래서 cost가 크면, `모델이 뭔가 잘못하고 있구나`라고 해석할 수 있는 것.

어떻게 구하는거냐면, 원하는 정답과 아웃풋의 뉴런별 차이값을 제곱해서 더해주는 것.

이게 코스트임(or loss)

loss 값이 적어지도록 하는 것이 중요함. (하나의 데이터에 대해서가 아니라, 모든 훈련 데이터에 대해서 평균 cost가 작은 것이 중요함)

> REMEMBER!!! 이걸 기억하라고 강조함. 우리가 학습하는 신경망이 실제로는 하나의 복잡한 함수라는 점.

이 함수(신경망)는

- input: 784개의 숫자(pixel값)
- output: 10개의 숫자(각 숫자가 0~9에 해당되는 값)
- parameters: 13,002 weights/biases

그러면 cost function은?

- input: 13,002 weights/biases
- output: 숫자 1개 = COST
- parameters: 수 많은 훈련 데이터.

그러면 그냥 기계 학습이라는 건, 간단하게 생각하면, 모델한테 `weight 하고 bias를 이렇게 고쳐야됨~` 하고 알려주는 것과 비슷함.

수 많은 파라미터를 가지고 생각하지말고 (그러면 어려우니까)

하나의 단일 인풋을 비용함수에 전달하는 구조로 생각해보자.

C(w)값이, 가장 최소가 되도록 하는 w값이 무엇인지 어떻게 알 수 있을까?

미분값이 0이 되는 지점을 찾으면 되는거아님? -> 값이 엄청 많아서 함수가 엄청 복잡하면 해당 지점을 항상 알아낸다는 보장이 없음.

대신 이런건 알 수 있음.

입력값이 주어졌을 때, 어떤 방향으로 이동해야지 값이 낮아지는 방향으로의 이동인지는 알아낼 수 있음. 위치에서의 기울기를 알아낼 수 있다면

- 기울기가 양수라면(즉 우상향하는 그래프라면) 왼쪽으로 이동해야지 값이 낮아지고
- 기울기가 음수이면(즉 우하향하는 그래프라면) 오른쪽으로 이동해야지 값이 낮아짐.

계속 반복하면 이 지역에서는 최소값인 지역 최소값은 찾을 수 있음.

근데 문제는 말 그대로 `지역 최소값`이라는 거임.

전체 함수 분포에서, 즉 전역 최소값이라는 보장이 없음.

전역 최소값을 구하는게 되게 어렵다.

그러면 컴퓨터가 이걸 계산할 수 있어야 함.

간단하게 말하면 이 계산법이,

- 내려가는 방향과
- 내려가는 길이를

알아낼 수 있어서 이걸 계속 반복하다보면 답에 가까워질 수 있음.

이 원리를 13,000개의 입력이 있는 함수에 대해서도 확장시켜서 적용해볼 수 있음.

그래디언트는 모델이 현재 위치에서 코스트를 가장 빠르게 낮추는 방향이 어디인지 알려주는 역할을 합니다. -> 그렇다고 함.

방향 지시: 코스트 함수의 그래디언트는 코스트가 가장 빠르게 증가하는 방향을 가리킵니다.역방향 이동: 가장 빠르게 이동하는 방향임 반대로 이동하면 코스트가 가장 빠르게 감소하는 방향임. 이게 경사하강법(지역 최소로 이동하는 방법...)

그러면 이 그래디언트 계산을 어떻게 효과적으로 하는지가 중요함. 이걸 역전파라는 방법을 활용해서 구할 수 있는 것 같음.

근데 그래디언트가 코스트 가장 빠르게 낮추는 방향이 어디인지 알려준다고 했잖아. 이거는 곧 어떤 가중치를 조정하는 것이 더 큰 영향을 끼치는지 알려주는 것이라고 생각하면 됨.(+ 방향, 숫자로 생각하면 양수인지 음수인지) 어떤 뉴런간 연결을 조정하는 것이 다른 연결을 조정하는 것보다 전체 코스트를 낮추는데 더 큰 영향을 주겠지 정도는 이해할 수 있었음.

이 짓거리를 하는 것의 의의는,

처음보는 이미지가 입력되었을 때 숫자를 잘 매칭시킨다는 것. 처음 보는 패턴을 읽어서 판단할 수 있따는 것.

`http://neuralnetworksanddeeplearning.com/` -> 공부를 위해서 추천한 사이트
